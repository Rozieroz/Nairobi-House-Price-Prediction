{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3ba683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Size extraction \n",
    "\n",
    "import re\n",
    "\n",
    "def extract_size_from_text(text):\n",
    "    \"\"\"\n",
    "    Extract realistic built-up/property size from messy real estate descriptions.\n",
    "    Supports sqm, m², square meters, sq ft, and acre.\n",
    "    \"\"\"\n",
    "\n",
    "    if not text:\n",
    "        return \"N/A\"\n",
    "\n",
    "    text = text.replace(\",\", \"\")\n",
    "    candidates = []\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 1. Ranges in sqm (e.g. 350 – 400 sqm, 465 to 476 sqm)\n",
    "    # ---------------------------------------------------\n",
    "    range_matches = re.findall(\n",
    "        r'(\\d+(\\.\\d+)?)\\s*(?:–|-|to)\\s*(\\d+(\\.\\d+)?)\\s*(sqm|m²|square meters?)',\n",
    "        text,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    for match in range_matches:\n",
    "        low = float(match[0])\n",
    "        high = float(match[2])\n",
    "        if high >= 30:  # realistic lower bound for Kenya\n",
    "            candidates.append((high, f\"{match[0]}–{match[2]} sqm\"))\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 2. Single sqm values\n",
    "    # ---------------------------------------------------\n",
    "    sqm_matches = re.findall(\n",
    "        r'(\\d+(\\.\\d+)?)\\s*(sqm|m²|square meters?)',\n",
    "        text,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    for match in sqm_matches:\n",
    "        value = float(match[0])\n",
    "        if value >= 30:  # ~ small studio size\n",
    "            candidates.append((value, f\"{match[0]} sqm\"))\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 3. Square feet (convert to sqm for comparison)\n",
    "    # ---------------------------------------------------\n",
    "    sqft_matches = re.findall(\n",
    "        r'(\\d+(\\.\\d+)?)\\s*(sq\\.?\\s*ft\\.?|sqft)',\n",
    "        text,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    for match in sqft_matches:\n",
    "        sqft_value = float(match[0])\n",
    "        if sqft_value >= 300:  # adjusted for Kenya\n",
    "            sqm_equivalent = sqft_value * 0.092903\n",
    "            candidates.append((sqm_equivalent, f\"{match[0]} sq ft\"))\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 4. Acres (convert to sqm for comparison)\n",
    "\n",
    "    acre_matches = re.findall(\n",
    "        r'(\\d+/\\d+|\\d+(\\.\\d+)?)\\s*-?\\s*(acre)',\n",
    "        text,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    for match in acre_matches:\n",
    "        raw_value = match[0]\n",
    "\n",
    "        if \"/\" in raw_value:\n",
    "            num, denom = raw_value.split(\"/\")\n",
    "            acre_value = float(num) / float(denom)\n",
    "        else:\n",
    "            acre_value = float(raw_value)\n",
    "\n",
    "        if acre_value >= 0.05:  # ignore unrealistic tiny plots\n",
    "            sqm_equivalent = acre_value * 4046.86\n",
    "            candidates.append((sqm_equivalent, f\"{raw_value} acre\"))\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 5. Return largest realistic size, and avoid small sizes that are likely not the property size\n",
    "    if candidates:\n",
    "        # Filter out candidates that are too small (e.g., less than 30 sqm)\n",
    "        filtered_candidates = [c for c in candidates if c[0] >= 30]\n",
    "        if filtered_candidates:\n",
    "            return max(filtered_candidates, key=lambda x: x[0])[1]\n",
    "\n",
    "    return \"N/A\"\n",
    "\n",
    "\n",
    "# ======================================\n",
    "# Fetch with retries\n",
    "\n",
    "def fetch_page(url, headers, retries=3):\n",
    "    \"\"\"Fetch a page with retries and error handling.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Attempt {attempt+1}: Status code {response.status_code}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Attempt {attempt+1}: Request failed - {e}\")\n",
    "        time.sleep(2)\n",
    "    return None\n",
    "\n",
    "# ======================================================\n",
    "# Extract from listing card\n",
    "\n",
    "def extract_property_type(title):\n",
    "\n",
    "    # Extract property type from title (e.g., 'Apartment', 'Townhouse')\n",
    "    title_lower = title.lower()\n",
    "    types = ['apartment', 'townhouse', 'bungalow', 'villa', 'mansion', 'house', 'land', 'commercial']\n",
    "    for t in types:\n",
    "        if t in title_lower:\n",
    "            return t.capitalize()\n",
    "    return 'Unknown'\n",
    "\n",
    "\n",
    "def extract_bedrooms_bathrooms_size(listing):\n",
    "    # Extract bedrooms, bathrooms, size from swiper slides and description on listing card\n",
    "    bedrooms = bathrooms = \"N/A\"\n",
    "    size_from_swiper = \"N/A\"\n",
    "\n",
    "    # 1. Swiper slides (bedrooms, bathrooms, quick size)\n",
    "    swiper_div = listing.find('div', class_='scrollable-list')\n",
    "    if swiper_div:\n",
    "        slides = swiper_div.find_all('div', class_='swiper-slide')\n",
    "        for slide in slides:\n",
    "            text = slide.get_text(strip=True)\n",
    "            if 'Bedroom' in text:\n",
    "                bedrooms = text\n",
    "            elif 'Bathroom' in text:\n",
    "                bathrooms = text\n",
    "            elif 'm²' in text or 'sq' in text.lower() or 'acre' in text.lower():\n",
    "                size_from_swiper = text\n",
    "\n",
    "    # 2. extract size from description on the listing card\n",
    "    size_from_desc = \"N/A\"\n",
    "    # Look for common description containers\n",
    "    desc_div = (listing.find('div', id='truncatedDescription') or \n",
    "                listing.find('div', class_='description') or\n",
    "                listing.find('p', class_='description'))\n",
    "    if desc_div:\n",
    "        desc_text = desc_div.get_text(\" \", strip=True)\n",
    "        size_from_desc = extract_size_from_text(desc_text)\n",
    "\n",
    "    # Prefer description-based size if found\n",
    "    if size_from_desc != \"N/A\":\n",
    "        size = size_from_desc\n",
    "    else:\n",
    "        size = size_from_swiper\n",
    "\n",
    "    return bedrooms, bathrooms, size\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# Extract from detail page\n",
    "\n",
    "def parse_detail_page(detail_soup):\n",
    "\n",
    "    # Extract creation date, amenities, nearby, and size from property detail page\n",
    "    utilities = []\n",
    "    nearby = []\n",
    "    created_at = 'N/A'\n",
    "    size_from_detail = \"N/A\"\n",
    "\n",
    "    # Creation date\n",
    "    created_tag = detail_soup.find(string=lambda x: x and \"Created At:\" in x)\n",
    "    if created_tag:\n",
    "        created_at = created_tag.strip().replace(\"Created At:\", \"\").strip()\n",
    "\n",
    "    # Size from detail page (full description)\n",
    "    # Look in common description containers first\n",
    "    desc_section = (detail_soup.find('div', class_='property-description') or \n",
    "                    detail_soup.find('div', class_='description') or\n",
    "                    detail_soup.find('section', class_='description'))\n",
    "    if desc_section:\n",
    "        desc_text = desc_section.get_text(\" \", strip=True)\n",
    "        size_from_detail = extract_size_from_text(desc_text)\n",
    "    else:\n",
    "        # Fallback: search entire page text\n",
    "        page_text = detail_soup.get_text(\" \", strip=True)\n",
    "        size_from_detail = extract_size_from_text(page_text)\n",
    "\n",
    "    # Amenities and nearby facilities\n",
    "    sections = detail_soup.find_all(\"div\", class_=\"px-3 py-3 even:bg-gray-50\")\n",
    "    for section in sections:\n",
    "        title_span = section.find(\"span\", class_=\"font-semibold\")\n",
    "        if not title_span:\n",
    "            continue\n",
    "\n",
    "        section_name = title_span.get_text(strip=True).lower()\n",
    "        items_div = section.find(\"div\", class_=\"flex flex-wrap gap-3\")\n",
    "        if not items_div:\n",
    "            continue\n",
    "\n",
    "        items = [span.get_text(strip=True) for span in items_div.find_all(\"span\") if span.get_text(strip=True)]\n",
    "        if \"internal features\" in section_name or \"external features\" in section_name:\n",
    "            utilities.extend(items)\n",
    "        elif \"nearby\" in section_name:\n",
    "            nearby.extend(items)\n",
    "\n",
    "    return created_at, utilities, nearby, size_from_detail\n",
    "\n",
    "# ======================================\n",
    "# Scrape one listing\n",
    "\n",
    "def scrape_listing(listing, base_url, headers):\n",
    "\n",
    "    # Scrape data from a single listing card and its detail page\n",
    "\n",
    "    # --- Basic info from card ---\n",
    "    title_tag = listing.find('h2')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else 'No title'\n",
    "\n",
    "    price_tag = listing.find('a', class_='pointer-events-none z-10 no-underline')\n",
    "    price = price_tag.get_text(strip=True) if price_tag else 'No price'\n",
    "\n",
    "    location_tag = listing.find('p', class_='w-full truncate font-normal capitalize')\n",
    "    location = location_tag.get_text(strip=True) if location_tag else 'No location'\n",
    "\n",
    "    property_type = extract_property_type(title)\n",
    "    bedrooms, bathrooms, size = extract_bedrooms_bathrooms_size(listing)\n",
    "\n",
    "    # --- Detail page URL ---\n",
    "    property_tag = listing.find('a', href=True)\n",
    "    if not property_tag:\n",
    "        return None\n",
    "    property_url = urljoin(base_url, property_tag['href'])\n",
    "\n",
    "    # --- Fetch detail page ---\n",
    "    detail_response = fetch_page(property_url, headers)\n",
    "    if not detail_response:\n",
    "        # Return basic info only\n",
    "        return {\n",
    "            'Title': title,\n",
    "            'Property Type': property_type,\n",
    "            'Price': price,\n",
    "            'Location': location,\n",
    "            'Bedrooms': bedrooms,\n",
    "            'Bathrooms': bathrooms,\n",
    "            'Size': size,\n",
    "            'Amenities': [],\n",
    "            'Surroundings': [],\n",
    "            'Created At': 'N/A',\n",
    "            'URL': property_url\n",
    "        }\n",
    "\n",
    "    detail_soup = BeautifulSoup(detail_response.content, 'html.parser')\n",
    "    created_at, utilities, nearby, size_from_detail = parse_detail_page(detail_soup)\n",
    "\n",
    "    # Override size with detail page version if found\n",
    "    if size_from_detail != \"N/A\":\n",
    "        size = size_from_detail\n",
    "\n",
    "    return {\n",
    "        'Title': title,\n",
    "        'Property Type': property_type,\n",
    "        'Price': price,\n",
    "        'Location': location,\n",
    "        'Bedrooms': bedrooms,\n",
    "        'Bathrooms': bathrooms,\n",
    "        'Size': size,\n",
    "        'Amenities': utilities,\n",
    "        'Surroundings': nearby,\n",
    "        'Created At': created_at,\n",
    "    }\n",
    "\n",
    "# ===============================================\n",
    "# Main scraping loop\n",
    "\n",
    "def scrape_pages(start_page, end_page, max_listings=800):\n",
    "    \"\"\"\n",
    "    Iterates over pages and listings, collects data, stops when max_listings reached.\n",
    "    \"\"\"\n",
    "    base_url = 'https://www.buyrentkenya.com/houses-for-sale'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    properties = []\n",
    "    page_num = start_page\n",
    "\n",
    "    while page_num <= end_page and len(properties) < max_listings:\n",
    "        url = f'{base_url}?page={page_num}'\n",
    "        print(f\"Scraping page {page_num}: {url}\")\n",
    "\n",
    "        response = fetch_page(url, headers)\n",
    "        if not response:\n",
    "            print(f\"Failed to retrieve page {page_num}, moving to next.\")\n",
    "            page_num += 1\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        listings = soup.find_all('div', class_='listing-card')\n",
    "\n",
    "        if not listings:\n",
    "            print(f\"No listings found on page {page_num}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        for listing in listings:\n",
    "            if len(properties) >= max_listings:\n",
    "                break\n",
    "            \n",
    "\n",
    "        # SCRAPPED DATA ======================\n",
    "            property_data = scrape_listing(listing, base_url, headers)\n",
    "            if property_data:\n",
    "                properties.append(property_data)\n",
    "                # print(f\"  Scraped: {property_data['Title']} ({len(properties)} total)\")\n",
    "\n",
    "            # Polite delay between detail page requests\n",
    "            time.sleep(1)\n",
    "\n",
    "        page_num += 1\n",
    "        time.sleep(2)  # delay between pages\n",
    "\n",
    "    print(f\"Scraping finished. Total listings collected: {len(properties)}\")\n",
    "    return pd.DataFrame(properties)\n",
    "\n",
    "# ========================\n",
    "# Save data functions\n",
    "\n",
    "def save_raw_data(df, filename='../data/raw_listings.csv'):\n",
    "    \"\"\"Save raw dataframe to CSV, creating directory if needed.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Raw data saved to {filename}\")\n",
    "\n",
    "def create_data_dictionary():\n",
    "    \"\"\"Return the data dictionary as a list of dicts.\"\"\"\n",
    "    return [\n",
    "        {\"Column Name\": \"Title\", \"Description\": \"Name of property listing\", \"Data Type\": \"String\", \"Example\": \"5 Bed Townhouse with En Suite in Nyali Area\"},\n",
    "        {\"Column Name\": \"Property Type\", \"Description\": \"Type of property (apartment, townhouse, etc.)\", \"Data Type\": \"String\", \"Example\": \"Townhouse\"},\n",
    "        {\"Column Name\": \"Price\", \"Description\": \"Listed price\", \"Data Type\": \"String / Numeric\", \"Example\": \"KSh 29,500,000\"},\n",
    "        {\"Column Name\": \"Location\", \"Description\": \"Property location\", \"Data Type\": \"String\", \"Example\": \"Nyali Area, Nyali\"},\n",
    "        {\"Column Name\": \"Bedrooms\", \"Description\": \"Number of bedrooms\", \"Data Type\": \"String / Int\", \"Example\": \"5 Bedrooms\"},\n",
    "        {\"Column Name\": \"Bathrooms\", \"Description\": \"Number of bathrooms\", \"Data Type\": \"String / Int\", \"Example\": \"5 Bathrooms\"},\n",
    "        {\"Column Name\": \"Size\", \"Description\": \"Property size with unit\", \"Data Type\": \"String\", \"Example\": \"250 m²\"},\n",
    "        {\"Column Name\": \"Amenities\", \"Description\": \"Internal & external features\", \"Data Type\": \"List (comma-separated in CSV)\", \"Example\": \"Aircon, Alarm, Backup Generator\"},\n",
    "        {\"Column Name\": \"Surroundings\", \"Description\": \"Nearby facilities / landmarks\", \"Data Type\": \"List (comma-separated in CSV)\", \"Example\": \"Bus Stop, Golf Course, Hospital\"},\n",
    "        {\"Column Name\": \"Created At\", \"Description\": \"Date listing was created\", \"Data Type\": \"Date / String\", \"Example\": \"09 February 2026\"},\n",
    "        {\"Column Name\": \"URL\", \"Description\": \"Link to the property page\", \"Data Type\": \"String\", \"Example\": \"https://www.buyrentkenya.com/...\"},\n",
    "    ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a415c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Main execution\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# def main():\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#     # Configuration\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#     # Save raw data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m save_raw_data(\u001b[43mdf\u001b[49m, \u001b[33m'\u001b[39m\u001b[33m../data/raw_listings.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Save data dictionary as JSON\u001b[39;00m\n\u001b[32m     23\u001b[39m data_dict = create_data_dictionary()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    # Configuration\n",
    "    START_PAGE = 1\n",
    "    END_PAGE = 40      \n",
    "    MAX_LISTINGS = 800     # max\n",
    "\n",
    "    # Scrape\n",
    "    df = scrape_pages(START_PAGE, END_PAGE, MAX_LISTINGS)\n",
    "\n",
    "    df = df.copy()\n",
    "    df['Amenities'] = df['Amenities'].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "    df['Surroundings'] = df['Surroundings'].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "    # Display sample data with flattened amenities and surroundings\n",
    "    print(\"Sample data with flattened amenities and surroundings:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Save raw data\n",
    "    save_raw_data(df, '../data/raw_listings.csv')\n",
    "\n",
    "    # Save data dictionary as JSON\n",
    "    data_dict = create_data_dictionary()\n",
    "    dd_df = pd.DataFrame(data_dict)\n",
    "    dd_df.to_json('../data/data_dictionary.json', orient='records', indent=2)\n",
    "    print(\"Data dictionary saved as data_dictionary.json\")\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n--- Summary ---\")\n",
    "    print(f\"Total listings: {len(df)}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "\n",
    "   \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
